import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("/content/drive/MyDrive/heart_attack_prediction_indonesia.csv")
data.head()

print(data.isnull().sum())

print(data.duplicated().sum())

print(data.dtypes)
print(data.nunique())

import matplotlib.pyplot as plt
import seaborn as sns

# EXPLORATORY DATA ANALYSIS
print("ðŸ” Dataset shape:", data.shape)

# 1. Missing values
print("\n Missing Values Per Column:")
print(data.isnull().sum())

# 2. Data types & non-numeric columns
print("\n Data Types:")
print(data.dtypes)

# 3. Quick look at unique values in some key categorical columns
categorical_cols = ['gender', 'region', 'alcohol_consumption', 'smoking_status',
                    'dietary_habits', 'physical_activity', 'medication_usage', 'heart_attack']

print("\n Unique Values in Selected Categorical Columns:")
for col in categorical_cols:
    print(f"\n{col}:")
    print(data[col].value_counts(dropna=False))  # Keep NaNs in the count

# 4. Summary stats for numerical columns
print("\n Statistical Summary (Numerical Columns):")
print(data.describe())

# 5. Histograms for numeric distributions
import matplotlib.pyplot as plt
import seaborn as sns
print("\nðŸ“Š Plotting histograms...")
data.hist(figsize=(18, 14), bins=30)
plt.tight_layout()
plt.show()

# 6. Target class balance
print("\n Heart Attack Target Distribution:")
print(data['heart_attack'].value_counts(normalize=True).rename_axis('heart_attack').reset_index(name='proportion'))

# 7. Correlation heatmap (optional, to get quick idea of relationships)
print("\n Correlation Heatmap:")
numeric_data = data.select_dtypes(include=['float64', 'int64'])
plt.figure(figsize=(14, 12))
sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# CORRELATION WITH THE TARGET VARIABLE
# Select only numeric columns
numeric_cols = data.select_dtypes(include=['int64', 'float64'])

# Calculate correlations with target
correlations = numeric_cols.corr()['heart_attack'].drop('heart_attack')

# Sort by absolute correlation
correlations_sorted = correlations.abs().sort_values(ascending=False)

# Show top 10 most correlated features
print("ðŸ” Top 10 Features Most Correlated with 'heart_attack':")
print(correlations.loc[correlations_sorted.index[:10]])

# DATA CLEANING
data['alcohol_consumption'] = data['alcohol_consumption'].fillna("zero")
print(data.isnull().sum())
data["alcohol_consumption"].head()

# REMOVE OUTLIERS
from scipy import stats

z_scores = stats.zscore(data_cleaned[numerical_cols])
outliers = (abs(z_scores) > 3)
outlier_counts = outliers.sum(axis=0)
rows_with_outliers = outliers.any(axis=1).sum()

print("Outlier counts per column:\n", outlier_counts)
print("\nTotal rows with at least one outlier:", rows_with_outliers)

from scipy import stats
cleaned_data = data.copy()

# Iteratively remove rows with any Z-score > 3
while True:
    z_scores = stats.zscore(cleaned_data[numerical_cols])
    outliers = (abs(z_scores) > 3)
    mask = ~outliers.any(axis=1)

    if mask.sum() == len(cleaned_data):  # No more outliers
        break
    cleaned_data = cleaned_data[mask]

# Update your main dataset
data = cleaned_data.copy()

# Confirm
z_scores_check = stats.zscore(data[numerical_cols])
outliers_check = (abs(z_scores_check) > 3)
rows_with_outliers_remaining = outliers_check.any(axis=1).sum()

print("Total rows with at least one outlier after iterative removal:", rows_with_outliers_remaining)

# ONE HOT ENCODING
data = pd.get_dummies(data, columns=['gender', 'region', 'income_level', 'smoking_status', 'alcohol_consumption', 'physical_activity', 'dietary_habits', 'air_pollution_exposure', 'stress_level', 'EKG_results'], drop_first=True)

# SMOTE 

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Step 1: Split your original dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 2: Apply SMOTE to training data only
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# Step 3: Train your model on the resampled data
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_sm, y_train_sm)

# Step 4: Predict and evaluate on the original test set
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

### LOGISTIC REGRESSION ###

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'data' is your preprocessed DataFrame and 'heart_attack' is the target column

# Define features (X) and target (y)
X = data.drop('heart_attack', axis=1)
y = data['heart_attack']

# Split data into training and testing sets
X_train_sm, X_test, y_train_sm, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize Logistic Regression model
log_reg = LogisticRegression(solver='liblinear', max_iter=1000) # Use 'liblinear' for smaller datasets or L1/L2 regularization

# Define a REDUCED parameter grid for Grid Search
# This grid has fewer combinations to speed up the search
param_grid = {
    'C': [0.1, 1, 10],  # Reduced number of C values
    'penalty': ['l2'] # Only using L2 regularization
}

# Initialize Grid Search with 5-fold Cross Validation
# cv=5 is kept as requested
# scoring='accuracy' is the metric used to evaluate each parameter combination
# n_jobs=-1 uses all available CPU cores for faster computation
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Train the model using Grid Search on the training data
# This step performs the cross-validation for each parameter combination
grid_search.fit(X_train_sm, y_train_sm)

# Get the best model from Grid Search
# This is the model with the hyperparameters that performed best during cross-validation
best_log_reg = grid_search.best_estimator_

# Make predictions on the test data using the best model
y_pred = best_log_reg.predict(X_test)

# Get the probability of the positive class (heart attack = 1) for AUC calculation
y_pred_proba = best_log_reg.predict_proba(X_test)[:, 1]

# Evaluate the model performance using various metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Generate a detailed classification report
classification_rep = classification_report(y_test, y_pred)

# Print the best parameters found by Grid Search
print("Best Parameters:", grid_search.best_params_)

# Print the model performance metrics
print("\nModel Performance Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"AUC-ROC: {roc_auc:.4f}")

# Print the confusion matrix
print("\nConfusion Matrix:")
print(conf_matrix)

# Print the classification report
print("\nClassification Report:")
print(classification_rep)

# Plot the Confusion Matrix for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Heart Attack', 'Heart Attack'], yticklabels=['No Heart Attack', 'Heart Attack'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()


### DECISION TREE ###

from sklearn.tree import DecisionTreeClassifier
# Assuming 'data' is your preprocessed DataFrame and 'heart_attack' is the target column
# And X_train, X_test, y_train, y_test are already defined from the previous split

# Initialize Decision Tree Classifier model
# Setting a random_state for reproducibility
dt_clf = DecisionTreeClassifier(random_state=42)

# Define parameter grid for Grid Search
# These are common hyperparameters for Decision Trees to tune
param_grid = {
    'max_depth': [None, 10, 20], # Maximum depth of the tree (None means no limit)
    'min_samples_split': [2, 10],   # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 4],     # Minimum number of samples required to be at a leaf node
    'criterion': ['gini']   # Function to measure the quality of a split
}

# Initialize Grid Search with 5-fold Cross Validation
# cv=5 is kept as requested
# scoring='accuracy' is the metric used to evaluate each parameter combination
# n_jobs=-1 uses all available CPU cores for faster computation
grid_search_dt = GridSearchCV(dt_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Train the model using Grid Search on the training data
grid_search_dt.fit(X_train_sm, y_train_sm)

# Get the best model from Grid Search
best_dt_clf = grid_search_dt.best_estimator_

# Make predictions on the test data using the best model
y_pred_dt = best_dt_clf.predict(X_test)

# Get the probability of the positive class for AUC calculation
y_pred_proba_dt = best_dt_clf.predict_proba(X_test)[:, 1]

# Evaluate the model performance
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)
roc_auc_dt = roc_auc_score(y_test, y_pred_proba_dt)
conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)
classification_rep_dt = classification_report(y_test, y_pred_dt)

# Print the best parameters
print("Best Parameters (Decision Tree):", grid_search_dt.best_params_)

# Print the performance metrics
print("\nModel Performance Metrics (Decision Tree):")
print(f"Accuracy: {accuracy_dt:.4f}")
print(f"Precision: {precision_dt:.4f}")
print(f"Recall: {recall_dt:.4f}")
print(f"F1-Score: {f1_dt:.4f}")
print(f"AUC-ROC: {roc_auc_dt:.4f}")

# Print the confusion matrix
print("\nConfusion Matrix (Decision Tree):")
print(conf_matrix_dt)

# Print the classification report
print("\nClassification Report (Decision Tree):")
print(classification_rep_dt)

# Plot the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Blues', xticklabels=['No Heart Attack', 'Heart Attack'], yticklabels=['No Heart Attack', 'Heart Attack'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Decision Tree')
plt.show()

### RANDOM FOREST ###

from sklearn.ensemble import RandomForestClassifier

# Assuming 'data' is your preprocessed DataFrame and 'heart_attack' is the target column
# And X_train, X_test, y_train, y_test are already defined from previous splits

# Initialize Random Forest Classifier model
# Setting a random_state for reproducibility
rf_clf = RandomForestClassifier(random_state=42)

# Define a REDUCED parameter grid for Grid Search
# Random Forests have several hyperparameters, so reducing the grid is crucial for speed
param_grid = {
    'n_estimators': [100], # Number of trees in the forest (reduced)
    'max_depth': [None, 10], # Maximum depth of the trees (reduced)
    'min_samples_split': [5], # Minimum number of samples required to split a node (reduced)
    'min_samples_leaf': [2], # Minimum number of samples required at each leaf node (reduced)
    'criterion': ['gini'] # Function to measure the quality of a split (keeping one for simplicity)
}

# Initialize Grid Search with 5-fold Cross Validation
# cv=5 is kept
# scoring='accuracy' is the metric used to evaluate each parameter combination
# n_jobs=-1 uses all available CPU cores for faster computation
grid_search_rf = GridSearchCV(rf_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Train the model using Grid Search on the training data
grid_search_rf.fit(X_train_sm, y_train_sm)

# Get the best model from Grid Search
best_rf_clf = grid_search_rf.best_estimator_

# Make predictions on the test data using the best model
y_pred_rf = best_rf_clf.predict(X_test)

# Get the probability of the positive class for AUC calculation
y_pred_proba_rf = best_rf_clf.predict_proba(X_test)[:, 1]

# Evaluate the model performance
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
classification_rep_rf = classification_report(y_test, y_pred_rf)

# Print the best parameters
print("Best Parameters (Random Forest):", grid_search_rf.best_params_)

# Print the performance metrics
print("\nModel Performance Metrics (Random Forest):")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-Score: {f1_rf:.4f}")
print(f"AUC-ROC: {roc_auc_rf:.4f}")

# Print the confusion matrix
print("\nConfusion Matrix (Random Forest):")
print(conf_matrix_rf)

# Print the classification report
print("\nClassification Report (Random Forest):")
print(classification_rep_rf)

# Plot the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['No Heart Attack', 'Heart Attack'], yticklabels=['No Heart Attack', 'Heart Attack'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

### MULTILAYERED PERCEPTRON ###

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler # MLP often benefits from scaling
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'data' is your preprocessed DataFrame and 'heart_attack' is the target column
# And X_train, X_test, y_train, y_test are already defined from previous splits

# --- Data Scaling (Crucial for MLP) ---
# Initialize StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train_sm)
X_test_scaled = scaler.transform(X_test)

# Initialize MLP Classifier model
# Setting a random_state for reproducibility
# max_iter is set to a reasonably high number to allow convergence
# early_stopping can be useful to prevent overfitting and speed up
mlp_clf = MLPClassifier(random_state=42, max_iter=1000, early_stopping=True)

# Define a parameter grid for Grid Search
# Tuning MLP hyperparameters can be complex and time-consuming
param_grid = {
    'hidden_layer_sizes': [(100,)], # Only one option for hidden layers
    'activation': ['relu'],                 # Only using 'relu' activation
    'solver': ['adam'],                             # Only using 'adam' solver
    'alpha': [0.0001],                       # Only one value for alpha
    'learning_rate': ['constant']       # Only using 'constant' learning rate
}

# Initialize Grid Search with 5-fold Cross Validation
# cv=5 is kept
# scoring='accuracy' is the metric used to evaluate each parameter combination
# n_jobs=-1 uses all available CPU cores for faster computation
grid_search_mlp = GridSearchCV(mlp_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Train the model using Grid Search on the SCALED training data
grid_search_mlp.fit(X_train_scaled, y_train_sm)

# Get the best model from Grid Search
best_mlp_clf = grid_search_mlp.best_estimator_

# Make predictions on the SCALED test data using the best model
y_pred_mlp = best_mlp_clf.predict(X_test_scaled)

# Get the probability of the positive class for AUC calculation
y_pred_proba_mlp = best_mlp_clf.predict_proba(X_test_scaled)[:, 1]

# Evaluate the model performance
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
precision_mlp = precision_score(y_test, y_pred_mlp)
recall_mlp = recall_score(y_test, y_pred_mlp)
f1_mlp = f1_score(y_test, y_pred_mlp)
roc_auc_mlp = roc_auc_score(y_test, y_pred_proba_mlp)
conf_matrix_mlp = confusion_matrix(y_test, y_pred_mlp)
classification_rep_mlp = classification_report(y_test, y_pred_mlp)

# Print the best parameters
print("Best Parameters (MLP):", grid_search_mlp.best_params_)

# Print the performance metrics
print("\nModel Performance Metrics (MLP):")
print(f"Accuracy: {accuracy_mlp:.4f}")
print(f"Precision: {precision_mlp:.4f}")
print(f"Recall: {recall_mlp:.4f}")
print(f"F1-Score: {f1_mlp:.4f}")
print(f"AUC-ROC: {roc_auc_mlp:.4f}")

# Print the confusion matrix
print("\nConfusion Matrix (MLP):")
print(conf_matrix_mlp)

# Print the classification report
print("\nClassification Report (MLP):")
print(classification_rep_mlp)

# Plot the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_mlp, annot=True, fmt='d', cmap='Blues', xticklabels=['No Heart Attack', 'Heart Attack'], yticklabels=['No Heart Attack', 'Heart Attack'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - MLP')
plt.show()

# COMPARISON OF THE MODELS USING PERFORMANCE METRICS

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming you already have these variables defined from your previous code:
# Logistic Regression: accuracy_lr, precision_lr, recall_lr, f1_lr, roc_auc_lr, y_pred_proba_lr
# Decision Tree: accuracy_dt, precision_dt, recall_dt, f1_dt, roc_auc_dt, y_pred_proba_dt
# Random Forest: accuracy_rf, precision_rf, recall_rf, f1_rf, roc_auc_rf, y_pred_proba_rf
# MLP: accuracy_mlp, precision_mlp, recall_mlp, f1_mlp, roc_auc_mlp, y_pred_proba_mlp

# Create a dictionary for easy handling
metrics = {
    "Logistic Regression": [accuracy_lr, precision_lr, recall_lr, f1_lr, roc_auc_lr],
    "Decision Tree": [accuracy_dt, precision_dt, recall_dt, f1_dt, roc_auc_dt],
    "Random Forest": [accuracy_rf, precision_rf, recall_rf, f1_rf, roc_auc_rf],
    "MLP": [accuracy_mlp, precision_mlp, recall_mlp, f1_mlp, roc_auc_mlp]
}

metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']

# Plot bar chart for metrics comparison
plt.figure(figsize=(12, 7))
bar_width = 0.2
x = range(len(metric_names))

for i, (model_name, values) in enumerate(metrics.items()):
    plt.bar([p + bar_width*i for p in x], values, width=bar_width, label=model_name)

plt.xticks([p + 1.5*bar_width for p in x], metric_names)
plt.ylim(0, 1.05)
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.legend()
plt.show()

# Plot ROC curves for all models
plt.figure(figsize=(10, 8))

fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_pred_proba_mlp)

plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.3f})')
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_dt:.3f})')
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.3f})')
plt.plot(fpr_mlp, tpr_mlp, label=f'MLP (AUC = {roc_auc_mlp:.3f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# CONFIDENCE LEVEL OF LOGISTIC REGRESSION

plt.figure(figsize=(8, 6))
plt.hist(y_pred_proba, bins=20, color='skyblue', edgecolor='black')
plt.title('Logistic Regression â€“ Prediction Confidence for Heart Attack (Class 1)')
plt.xlabel('Predicted Probability for Heart Attack')
plt.ylabel('Number of Samples')
plt.grid(True)
plt.show()

import numpy as np

# Average confidence for predicting heart attack (class 1)
avg_confidence_class1 = np.mean(y_pred_proba)
print(f"Average Confidence for Heart Attack Predictions (Class 1): {avg_confidence_class1:.4f}")

# CONFIDENCE LEVEL OF DECISION TREE

plt.figure(figsize=(8, 6))
plt.hist(y_pred_proba_dt, bins=20, color='skyblue', edgecolor='black')
plt.title('Decision Tree â€“ Prediction Confidence for Heart Attack (Class 1)')
plt.xlabel('Predicted Probability for Heart Attack')
plt.ylabel('Number of Samples')
plt.grid(True)
plt.show()

import numpy as np

# Average confidence for predicting heart attack (class 1)
avg_confidence_class1 = np.mean(y_pred_proba_dt)
print(f"Average Confidence for Heart Attack Predictions (Class 1): {avg_confidence_class1:.4f}")

# CONFIDENCE LEVEL OF RANDOM FOREST

plt.figure(figsize=(8, 6))
plt.hist(y_pred_proba_rf, bins=20, color='skyblue', edgecolor='black')
plt.title('Random Forest â€“ Prediction Confidence for Heart Attack (Class 1)')
plt.xlabel('Predicted Probability for Heart Attack')
plt.ylabel('Number of Samples')
plt.grid(True)
plt.show()

import numpy as np

# Average confidence for predicting heart attack (class 1)
avg_confidence_class1 = np.mean(y_pred_proba_rf)
print(f"Average Confidence for Heart Attack Predictions (Class 1): {avg_confidence_class1:.4f}")

# CONFIDENCE LEVEL OF MLP

plt.figure(figsize=(8, 6))
plt.hist(y_pred_proba_mlp, bins=20, color='skyblue', edgecolor='black')
plt.title('MLP â€“ Prediction Confidence for Heart Attack (Class 1)')
plt.xlabel('Predicted Probability for Heart Attack')
plt.ylabel('Number of Samples')
plt.grid(True)
plt.show()

import numpy as np

# Average confidence for predicting heart attack (class 1)
avg_confidence_class1 = np.mean(y_pred_proba_mlp)
print(f"Average Confidence for Heart Attack Predictions (Class 1): {avg_confidence_class1:.4f}")

# FEATURE IMPORTANCE FOR LOGISTIC REGRESSION - COEFFICIENTS

import numpy as np
import pandas as pd

coefficients = best_log_reg.coef_[0]  # Coefficients for each feature
feature_names = X.columns

coef_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients,
    'Odds Ratio': np.exp(coefficients)
})

# Sort by absolute value of coefficients
coef_df['Abs Coef'] = np.abs(coef_df['Coefficient'])
coef_df = coef_df.sort_values(by='Abs Coef', ascending=False)

print("\nLogistic Regression Feature Importance (Top Features):")
print(coef_df[['Feature', 'Coefficient', 'Odds Ratio']].head(10))

# FEATURE IMPORTANCE FOR DECISION TREE - IMPORTANCE SCORE

import pandas as pd

feature_importances_dt = best_dt_clf.feature_importances_
feature_names = X.columns

dt_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances_dt
})

# Sort by importance
dt_importance_df = dt_importance_df.sort_values(by='Importance', ascending=False)

print("\nFeature Importance (Decision Tree):")
print(dt_importance_df.head(10))

# Optional: Plot the top 10 important features
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=dt_importance_df.head(10), palette='viridis')
plt.title('Top 10 Important Features â€“ Decision Tree')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# FEATURE IMPORTANCE FOR RANDOM FOREST - IMPORTANCE SCORE

importances_rf = best_rf_clf.feature_importances_

# Sort the features by importance (descending order)
indices_rf = importances_rf.argsort()[::-1]

# Print the feature ranking
print("Feature ranking (Random Forest):")
for i in indices_rf:
    print(f"{X.columns[i]}: {importances_rf[i]:.4f}")

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Random Forest")
plt.bar(range(len(importances_rf)), importances_rf[indices_rf], color="lightgreen", align="center")
plt.xticks(range(len(importances_rf)), X.columns[indices_rf], rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance Score')
plt.tight_layout()
plt.show()

# FEATURE IMPORTANCE FOR MLP - SHAP VALUES

import shap
import numpy as np

# background and test data as before
background = X_train_scaled[:100]
test_samples = X_test_scaled[:50]

# Define a wrapper function that returns predict_proba output
def model_predict_proba(X):
    return best_mlp_clf.predict_proba(X)

# Create SHAP explainer with this wrapper function and background data
explainer = shap.Explainer(model_predict_proba, background, feature_names=X_train_sm.columns)

# Get SHAP values for test data
shap_values = explainer(test_samples)

# SHAP values shape: (samples, features, classes)
print("SHAP values shape:", shap_values.values.shape)

# Plot summary plot for the positive class (index 1), with feature names
shap.summary_plot(shap_values.values[:, :, 1], test_samples, feature_names=X_train_sm.columns)

